{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a67813-3fc6-457a-9313-4d47ba3144ba",
   "metadata": {},
   "source": [
    "<h1>Data Extraction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c85838-772e-4a5b-a788-3831ae991ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "a91c4e8f-2086-4088-b4c5-ca86829637a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file names\n",
    "SCHOOL_FED = \"federal_college_data.csv\"\n",
    "SCHOOL_FORBES = \"forbes_rankings.csv\"\n",
    "SCHOOL_RATINGS = \"myplan_rankings.csv\"\n",
    "CITY_ZIPS = \"us_cities_zip_county.csv\"\n",
    "CITY_ZIPS_JSON = \"us_cities_zip_county.json\"\n",
    "CITY_CRIME = \"us_cities_crime.csv\"\n",
    "CITY_POP = \"us_cities_pop.csv\" # not used in final analysis\n",
    "# Zippopotam API used in the data_integration code to match zips to cities in the CITY_CRIME file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2edc5a17-b7d8-4bce-bf0d-c249bd90f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_ABBREVS = {\n",
    "    \"alabama\": \"AL\",\n",
    "    \"alaska\": \"AK\",\n",
    "    \"arizona\": \"AZ\",\n",
    "    \"arkansas\": \"AR\",\n",
    "    \"california\": \"CA\",\n",
    "    \"colorado\": \"CO\",\n",
    "    \"connecticut\": \"CT\",\n",
    "    \"delaware\": \"DE\",\n",
    "    \"florida\": \"FL\",\n",
    "    \"georgia\": \"GA\",\n",
    "    \"hawaii\": \"HI\",\n",
    "    \"idaho\": \"ID\",\n",
    "    \"illinois\": \"IL\",\n",
    "    \"indiana\": \"IN\",\n",
    "    \"iowa\": \"IA\",\n",
    "    \"kansas\": \"KS\",\n",
    "    \"kentucky\": \"KY\",\n",
    "    \"louisiana\": \"LA\",\n",
    "    \"maine\": \"ME\",\n",
    "    \"maryland\": \"MD\",\n",
    "    \"massachusetts\": \"MA\",\n",
    "    \"michigan\": \"MI\",\n",
    "    \"minnesota\": \"MN\",\n",
    "    \"mississippi\": \"MS\",\n",
    "    \"missouri\": \"MO\",\n",
    "    \"montana\": \"MT\",\n",
    "    \"nebraska\": \"NE\",\n",
    "    \"nevada\": \"NV\",\n",
    "    \"new hampshire\": \"NH\",\n",
    "    \"new jersey\": \"NJ\",\n",
    "    \"new mexico\": \"NM\",\n",
    "    \"new york\": \"NY\",\n",
    "    \"north carolina\": \"NC\",\n",
    "    \"north dakota\": \"ND\",\n",
    "    \"ohio\": \"OH\",\n",
    "    \"oklahoma\": \"OK\",\n",
    "    \"oregon\": \"OR\",\n",
    "    \"pennsylvania\": \"PA\",\n",
    "    \"rhode island\": \"RI\",\n",
    "    \"south carolina\": \"SC\",\n",
    "    \"south dakota\": \"SD\",\n",
    "    \"tennessee\": \"TN\",\n",
    "    \"texas\": \"TX\",\n",
    "    \"utah\": \"UT\",\n",
    "    \"vermont\": \"VT\",\n",
    "    \"virginia\": \"VA\",\n",
    "    \"washington\": \"WA\",\n",
    "    \"west virginia\": \"WV\",\n",
    "    \"wisconsin\": \"WI\",\n",
    "    \"wyoming\": \"WY\",\n",
    "    \"district of columbia\": \"DC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9c8b2-849e-4bc0-82cb-5682a7163e93",
   "metadata": {},
   "source": [
    "<h2>Validate CSV File Name</h2>\n",
    "All data will be saved to CSV files upon intial extraction. The function below is used in scraping of all three sources to verify that the input filename is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e261f3d8-4a64-4ef4-85eb-48abe5829d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_filename(filename: str) -> bool:\n",
    "\n",
    "    # check for correct file extension\n",
    "    if not filename.lower().endswith(\".csv\"):\n",
    "        print(\"Error: Filename must end with .csv\")\n",
    "        return False\n",
    "\n",
    "    # check for invalid characters\n",
    "    invalid_characters = r'[<>:\"/\\\\|?*\\']'\n",
    "    if re.search(invalid_characters, filename):\n",
    "        print(\"Error: Filename contains invalid characters.\")\n",
    "        return False\n",
    "\n",
    "    # check for empty or whitespace-only name\n",
    "    if filename.strip() == \".csv\":\n",
    "        print(\"Error: Filename cannot be empty or just whitespace.\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d6916096-8543-4c72-9ddc-133cc5a470ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Filename cannot be empty or just whitespace.\n",
      "Error: Filename must end with .csv\n",
      "Error: Filename contains invalid characters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test file name verification function\n",
    "validate_filename(\".csv\")\n",
    "validate_filename(\"colleges\")\n",
    "validate_filename(\"colleges?.csv\")\n",
    "validate_filename(\"colleges.CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73966f01-9d5a-4a89-8d7a-08ad50e19cae",
   "metadata": {},
   "source": [
    "<h2>Forbes Rankings</h2>\n",
    "<h3>Variables of Interest</h3>\n",
    "Institution Name, Rank, State, Average Grade, Median Base Salary, Student Population, Campus Setting, School Size, Description, Institution Type, Carnegie Classification, Student to Faculty Ratio, Total Grant Aid, Percent of Students Receive Financial Aid, Percent of Students Receive Grants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "f7e060eb-5851-452c-bfa0-0c11d0e344cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forbes_data(filename: str):\n",
    "    # JSON API endpoint url -- obtained from browserâ€™s Network tab in Developer Tools\n",
    "    # original data url - https://www.forbes.com/top-colleges/\n",
    "    FORBES_URL = \"https://www.forbes.com/forbesapi/org/top-colleges/2025/rank/true.json?fields=organizationName,academics,state,financialAid,rank,medianBaseSalary,campusSetting,studentPopulation,squareImage,uri,description,grade,schoolSize&limit=500&start=0\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    universities = []\n",
    "    \n",
    "    # validate filename to ensure that the data can be downloaded into a CSV file\n",
    "    if validate_filename(filename):\n",
    "        pass\n",
    "    else:\n",
    "        return f\"Invalid filename: {filename}. Please check your filename and try again.\"\n",
    "    \n",
    "    # call page and show error if unable to make the request\n",
    "    try:\n",
    "        response = requests.get(FORBES_URL, headers=headers)\n",
    "        data = response.json()\n",
    "        schools = data['organizationList']['organizationsLists']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # helps diagnose issues with HTTP request\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "    except KeyError as e:\n",
    "        # helps diagnose API structure changes\n",
    "        print(f\"Key error: {e}\")\n",
    "        print(\"Available keys in response:\", data.keys())\n",
    "        return None\n",
    "    \n",
    "    # loop through schools and collect data\n",
    "    for school in schools:\n",
    "        university = {\n",
    "            'rank': school.get('rank'),\n",
    "            'name': school.get('organizationName'),\n",
    "            'state': school.get('state'),\n",
    "            'grade': school.get('grade'),\n",
    "            'medianBaseSalary': school.get('medianBaseSalary'),\n",
    "            'studentPopulation': school.get('studentPopulation'),\n",
    "            'campusSetting': school.get('campusSetting'),\n",
    "            'schoolSize': school.get('schoolSize'),\n",
    "            'description': school.get('description'),\n",
    "            'uri': school.get('uri')\n",
    "        }\n",
    "    \n",
    "        academics = school.get('academics', {})\n",
    "        university['institutionType'] = academics.get('type')\n",
    "        university['carnegieClassification'] = academics.get('carnegieClassification')\n",
    "        university['studentFacultyRatio'] = academics.get('studentFacultyRatio')\n",
    "    \n",
    "        financial_aid = school.get('financialAid', {})\n",
    "        university['totalGrantAid'] = financial_aid.get('totalGrantAid')\n",
    "        university['percentOfStudentsFinAid'] = financial_aid.get('percentOfStudentsFinAid')\n",
    "        university['percentOfStudentsGrant'] = financial_aid.get('percentOfStudentsGrant')\n",
    "        \n",
    "        # add all institution data to universities list, which can then be appended to a CSV file\n",
    "        universities.append(university)\n",
    "    \n",
    "    # create dataframe from university data and extract into a CSV file\n",
    "    df = pd.DataFrame(universities)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    return f\"{len(universities)} universities added to {filename}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "0dd1d838-9ba7-4442-9297-a0ad03733ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'500 universities added to forbes_rankings.csv.'"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_forbes_data(SCHOOL_FORBES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6883a4d-ad07-47aa-b4b0-c825bee625df",
   "metadata": {},
   "source": [
    "<h2>myPlan Data</h2>\n",
    "<h3>Variables of Interest</h3>\n",
    "Institution Name, Prestige, Satisfaction, Resources & Facilities, Personal Safety, Teacher Support and Involvement, School Administration, Campus Setting, Aggregate score of all variables (Average Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "413ecd99-4daa-4166-bb71-8126f48318f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_myplan_data(url: str, entries: int):\n",
    "    # offset used to browse through page URLs\n",
    "    # an offset of 0 lists schools from 1-100, an offset of 400 lists schools from 401-500\n",
    "    # to get all instiutions, the amount of entries per variable will be input by the user to account for an increase of schools in the future\n",
    "    # ex. if 596 entries, offset would go up to 500 to list schools from 501-596\n",
    "    offset = [\"0\"]\n",
    "    entries = str(entries)\n",
    "    \n",
    "    def get_offset():\n",
    "        count = 1\n",
    "        if len(entries) == 1 or len(entries) == 2:\n",
    "            pass\n",
    "        elif len(entries) == 3:\n",
    "            while count <= int(entries[0]):\n",
    "                offset.append(str(count) + \"00\")\n",
    "                count += 1 \n",
    "        elif len(entries) == 4:\n",
    "            while count <= int(entries[0:2]):\n",
    "                offset.append(str(count) + \"00\")\n",
    "                count += 1 \n",
    "\n",
    "    get_offset()   \n",
    "    \n",
    "    university_rankings = []\n",
    "    \n",
    "    for i in offset:\n",
    "        page_url = url + i\n",
    "\n",
    "        # access url\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Prevents blocking\n",
    "        html = requests.get(page_url, headers=headers).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # loacte part of the HTML structure that contains university data\n",
    "        td_element = soup.find(\"td\", {\"background\": \"../../images/career_details_panel_bg_long.gif\"})\n",
    "\n",
    "        if td_element:\n",
    "            rows = td_element.find_all(\"tr\")\n",
    "            for row in rows:\n",
    "                cells = row.find_all(\"td\")\n",
    "                if len(cells) >= 3: \n",
    "                    name_tag = cells[1].find(\"a\", class_=\"ratings_list\")\n",
    "                    score_tag = cells[2].find(\"div\", align=\"right\")\n",
    "                    \n",
    "                    if name_tag and score_tag:\n",
    "                        name = name_tag.get_text(strip=True)\n",
    "                        score = score_tag.get_text(strip=True)\n",
    "                        try: #skip invalid scores\n",
    "                            float(score)\n",
    "                            university_rankings.append((name, score))\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "        else:\n",
    "            print(f\"No data found for offset {i}\")\n",
    "    \n",
    "    print(f\"{len(university_rankings)} total entries were found and documented.\")\n",
    "    return university_rankings\n",
    "\n",
    "\n",
    "# merge data from all school variables into a csv sheet\n",
    "# calculate avg score based on all variables\n",
    "\n",
    "def merge_and_save_data(variables_list:list, filename:str):\n",
    "    # define headers at the top of function for increased extensibility\n",
    "    HEADERS = [\n",
    "        \"School\", \"Prestige\", \"Satisfaction\", \"Resources & Facilities\",\n",
    "        \"Safety\", \"Teacher Support\", \"School Administration\", \"Campus Setting\", \"Average Score\"\n",
    "    ]\n",
    "    \n",
    "    # validate CSV file name\n",
    "    if validate_filename(filename):\n",
    "        pass\n",
    "    else:\n",
    "        return f\"Invalid filename: {filename}. Please check your filename and try again.\"\n",
    "    \n",
    "    num_variables = len(variables_list)\n",
    "    merged = {}\n",
    "    \n",
    "    # loop through each variable in ranking list and its index\n",
    "    # adds data into a dictionary with school name and associated scores\n",
    "    for idx, ranking_list in enumerate(variables_list):\n",
    "        for name, score in ranking_list:\n",
    "            if name not in merged:\n",
    "                merged[name] = [None] * num_variables  # initialize with None to signal missing values\n",
    "            merged[name][idx] = float(score)\n",
    "    \n",
    "    # convert to list of tuples: (School Name, score1, ..., score7, average)\n",
    "    merged_list = []\n",
    "    for name, scores in merged.items():\n",
    "        non_none_scores = [score for score in scores if score is not None]\n",
    "        if non_none_scores:\n",
    "            avg_score = sum(non_none_scores) / len(non_none_scores)\n",
    "        else:\n",
    "            avg_score = None\n",
    "        merged_list.append((name, *scores, round(avg_score, 2)))\n",
    "    \n",
    "    # save as csv\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(HEADERS)\n",
    "        writer.writerows(merged_list)\n",
    "\n",
    "    print(f\"Saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "4c46bc37-9cd5-491c-bf06-342870013bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611 total entries were found and documented.\n",
      "613 total entries were found and documented.\n",
      "611 total entries were found and documented.\n",
      "612 total entries were found and documented.\n",
      "611 total entries were found and documented.\n",
      "610 total entries were found and documented.\n",
      "613 total entries were found and documented.\n"
     ]
    }
   ],
   "source": [
    "# get myPlan ranking data from unique page per variable\n",
    "\n",
    "variable_sources = [\n",
    "    (\"prestige\", \"https://www.myplan.com/education/colleges/college_rankings_8.php?sort=1&offset=\", 611),\n",
    "    (\"satisfaction\", \"https://www.myplan.com/education/colleges/college_rankings_1.php?sort=1&offset=\", 613),\n",
    "    (\"resources\", \"https://www.myplan.com/education/colleges/college_rankings_4.php?sort=1&offset=\", 611),\n",
    "    (\"safety\", \"https://www.myplan.com/education/colleges/college_rankings_5.php?sort=1&offset=\", 612),\n",
    "    (\"teacher_support\", \"https://www.myplan.com/education/colleges/college_rankings_6.php?sort=1&offset=\", 611),\n",
    "    (\"school_admin\", \"https://www.myplan.com/education/colleges/college_rankings_7.php?sort=1&offset=\", 610),\n",
    "    (\"campus\", \"https://www.myplan.com/education/colleges/college_rankings_2.php?sort=1&offset=\", 613)\n",
    "]\n",
    "\n",
    "rankings = {}\n",
    "\n",
    "for name, url, entries in variable_sources:\n",
    "    rankings[name] = get_myplan_data(url, entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "612834b9-f0b3-472e-a237-de52503dc8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to myplan_rankings.csv\n"
     ]
    }
   ],
   "source": [
    "# merge data and save to csv file\n",
    "# 611 entries total\n",
    "merge_and_save_data(\n",
    "    [rankings[\"prestige\"], rankings[\"satisfaction\"], rankings[\"resources\"],\n",
    "     rankings[\"safety\"], rankings[\"teacher_support\"], rankings[\"school_admin\"],\n",
    "     rankings[\"campus\"]], \n",
    "    SCHOOL_RATINGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25714d0c-9d3b-4a9c-b20b-ee035df45d75",
   "metadata": {},
   "source": [
    "<h2>Department of Education API</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a4954b3-0b7a-4a9c-b713-f41fd96ff6c0",
   "metadata": {},
   "source": [
    "<h3>Variables Of Interest</h3>\n",
    "Name - name - INSTNM (TEXT) <br>\n",
    "State - school.state (TEXT) <br>\n",
    "Admission rate - admission_rate.overall - ADM_RATE (FLOAT) <br>  \n",
    "SAT Scores - admissions.sat_scores.average.overall <br>\n",
    "Enrollment of all undergraduate students - enrollment.all - UG (INT)  <br> \n",
    "Average net price for Title IV institutions (public institutions) - avg_net_price.public - NPT4_PUB (INT) <br>  \n",
    "Average net price for Title IV institutions (private for-profit and nonprofit institutions) - avg_net_price.private - NPT4_PRIV (INT)<br> \n",
    "Average cost of attendance (academic year institutions) - attendance.academic_year - COSTT4_A (INT)<br> \n",
    "The median debt for students who have completed - median_debt.completers.overall - GRAD_DEBT_MDN (FLOAT)<br> \n",
    "Median earnings of students working and not enrolled 8 years after entry - 8_yrs_after_entry.median_earnings - MD_EARN_WNE_P8 (FLOAT) <br>  \n",
    "Mean earnings of students working and not enrolled 8 years after entry - 8_yrs_after_entry.mean_earnings - MN_EARN_WNE_P8 (FLOAT)  <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b0bef-3a57-4106-9a9e-deb568083371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request\n",
    "# https://api.data.gov/ed/collegescorecard/v1/schools?api_key=YOUR_API_KEY&fields=id,school.name,latest.cost.tuition.in_state,latest.completion.rate\n",
    "\n",
    "# to request API key, visit: https://collegescorecard.ed.gov/data/api-documentation\n",
    "# API default rate limit - 1,000 requests per IP address per hour\n",
    "load_dotenv()  # Load from .env\n",
    "secret_key = os.getenv(\"API_KEY\")\n",
    "API_KEY = secret_key\n",
    "BASE_URL = \"https://api.data.gov/ed/collegescorecard/v1/schools\"\n",
    "\n",
    "# variables of interest [add here to include additional variables in research]\n",
    "FIELDS = [\n",
    "    \"school.zip\",\n",
    "    \"school.name\",\n",
    "    \"school.city\",\n",
    "    \"school.state\",\n",
    "    \"latest.school.locale\",\n",
    "    \"latest.school.faculty_salary\",\n",
    "    \"latest.admissions.sat_scores.average.overall\",\n",
    "    \"latest.admissions.admission_rate.overall\",\n",
    "    \"latest.completion.title_iv.completed_by.4yrs\",\n",
    "    \"latest.cost.avg_net_price.public\",\n",
    "    \"latest.cost.avg_net_price.private\",\n",
    "    \"latest.cost.attendance.academic_year\",\n",
    "    \"latest.aid.median_debt.completers.overall\",\n",
    "    \"latest.earnings.8_yrs_after_entry.median_earnings\",\n",
    "    \"latest.earnings.8_yrs_after_entry.mean_earnings\"\n",
    "]\n",
    "\n",
    "HEADERS = [\n",
    "    \"Zip Code\",\n",
    "    \"School Name\",\n",
    "    \"City\",\n",
    "    \"State\",\n",
    "    \"Locale\",\n",
    "    \"Average Faculty Salary\",\n",
    "    \"Average SAT Score\",\n",
    "    \"Admission Rate\",\n",
    "    \"4-Year Completion Rate\",\n",
    "    \"Average Net Price (Public)\",\n",
    "    \"Average Net Price (Private)\",\n",
    "    \"Cost of Attendance (Academic Year)\",\n",
    "    \"Median Debt of Completers\",\n",
    "    \"Median Earnings (8 Years After Entry)\",\n",
    "    \"Mean Earnings (8 Years After Entry)\"\n",
    "]\n",
    "\n",
    "def get_data_to_csv(start_page, end_page, filename):\n",
    "    # validate CSV file name\n",
    "    if validate_filename(filename):\n",
    "        pass\n",
    "    else:\n",
    "        return f\"Invalid filename: {filename}. Please check your file name and try again.\"\n",
    "    \n",
    "    all_data = []\n",
    "    page = start_page\n",
    "    \n",
    "    while page < end_page:\n",
    "        params = {\n",
    "            \"api_key\": API_KEY,\n",
    "            \"fields\": \",\".join(FIELDS),\n",
    "            \"per_page\": 100,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        results = data.get(\"results\", [])\n",
    "\n",
    "        def safe_get(school, field):\n",
    "            value = school.get(field)\n",
    "            return value if value is not None else \"N/A\"\n",
    "        \n",
    "        for school in results:\n",
    "            row = [safe_get(school, field) for field in FIELDS]\n",
    "            all_data.append(row)\n",
    "\n",
    "        page += 1\n",
    "    \n",
    "    # after fetching all data, save to CSV with human readable headers\n",
    "    # there are thousands of institutions in this API, so we will extract data in portions to check for error\n",
    "    # use append mode to add data at the end of the CSV file to prevent overwriting previous entries\n",
    "    file_exists = os.path.isfile(filename) # use os to prevent rewriting headers\n",
    "    with open(filename, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if not file_exists:\n",
    "            writer.writerow(HEADERS)\n",
    "        writer.writerows(all_data)\n",
    " \n",
    "    print(f\"Data from pages {start_page} to {end_page} saved to '{filename} - {len(all_data)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "84a45846-4d3a-415b-afab-9a5261a73e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from pages 0 to 10 saved to 'federal_college_data.csv - 1000 records\n",
      "Data from pages 10 to 20 saved to 'federal_college_data.csv - 1000 records\n",
      "Data from pages 20 to 30 saved to 'federal_college_data.csv - 1000 records\n",
      "Data from pages 30 to 40 saved to 'federal_college_data.csv - 1000 records\n",
      "Data from pages 40 to 50 saved to 'federal_college_data.csv - 1000 records\n",
      "Data from pages 50 to 60 saved to 'federal_college_data.csv - 1000 records\n",
      "Data from pages 60 to 65 saved to 'federal_college_data.csv - 429 records\n"
     ]
    }
   ],
   "source": [
    "# run in batches to check for errors and prevent overloading API\n",
    "# 6384 school entries total\n",
    "\n",
    "intervals = [(0,10), (10,20), (20,30), (30,40), (40,50), (50,60), (60,65)]\n",
    "\n",
    "for i in intervals:\n",
    "    get_data_to_csv(i[0], i[1], SCHOOL_FED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09502e6d-0761-45a1-b0f6-dc6951e84643",
   "metadata": {},
   "source": [
    "<h2>US City Population Data</h2>\n",
    "US Cities by State and their population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3280fec5-b3b4-4ccd-94af-40d71efca296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_info(state, abbrev, filename):\n",
    "    # validate CSV file name\n",
    "    if validate_filename(filename):\n",
    "        pass\n",
    "    else:\n",
    "        return f\"Invalid filename: {filename}. Please check your file name and try again.\"\n",
    "    \n",
    "    url = f\"https://www.geonames.org/postal-codes/US/{abbrev}/{state}.html\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find(\"table\", {\"class\": \"restable\"})\n",
    "    rows = table.find_all(\"tr\")[1:]  # Skip header row\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) >= 5:\n",
    "            city = cells[1].get_text(strip=True)\n",
    "            postal_code = cells[2].get_text(strip=True)\n",
    "            state = cells[4].get_text(strip=True)\n",
    "            county = cells[5].get_text(strip=True)\n",
    "    \n",
    "            data.append({\n",
    "                \"City\": city,\n",
    "                \"Postal Code\": postal_code,\n",
    "                \"State\": state,\n",
    "                \"County\": county,\n",
    "            })\n",
    "    \n",
    "    try:\n",
    "        file_exists = os.path.isfile(filename)\n",
    "    \n",
    "        with open(filename, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "            \n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "                \n",
    "            writer.writerows(data)\n",
    "        return f\"Data from {state} has been added to {filename}!\"\n",
    "    except:\n",
    "        return f\"Unable to add {state} data to {filename}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "c810c078-2128-4975-91e9-333ec2b74522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data as csv\n",
    "for state, abbrev in STATE_ABBREVS.items():\n",
    "    get_city_info(state, abbrev, CITY_ZIPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "e8732549-4104-44cf-8b67-59f37175b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert CITY_ZIPS file to JSON for quicker assess when merging data sets\n",
    "def csv_to_json(csv_filepath, json_filepath):\n",
    "    data = []\n",
    "    with open(csv_filepath, 'r') as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        for row in csv_reader:\n",
    "            data.append(row)\n",
    "\n",
    "    with open(json_filepath, 'w') as jsonfile:\n",
    "        json.dump(data, jsonfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3544b-2195-4075-aa58-b42e72b734cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_json(CITY_ZIPS, CITY_ZIPS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac2f60b6-993b-477e-b450-33b375dcee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this source was obtained, but not used because I found that my crime statistics data source also had population data\n",
    "#this source was redundant for my research, so i excluded it from my analysis\n",
    "\n",
    "def get_city_pop(state, filename):\n",
    "    # validate CSV file name\n",
    "    if validate_filename(filename):\n",
    "        pass\n",
    "    else:\n",
    "        return f\"Invalid filename: {filename}. Please check your file name and try again.\"\n",
    "    \n",
    "    # make custom state URL and get data\n",
    "    url = f\"https://www.city-data.com/city/{state}.html\"\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    \n",
    "    # access table from custom url\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\", id=\"cityTAB\")\n",
    "\n",
    "    if not table:\n",
    "        return f\"No data table found for {state}.\"\n",
    "\n",
    "    rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) >= 3:\n",
    "            city_name = cells[1].get_text(strip=True)\n",
    "            city_name_clean = city_name.split(\",\")[0]\n",
    "            population = cells[2].get_text(strip=True).replace(',', '')  # Remove commas\n",
    "            data.append({\"State\": state, \"City\": city_name_clean, \"Population\": population})\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(data)\n",
    "        file_exists = os.path.isfile(filename)\n",
    "        df.to_csv(filename, mode='a', header=not file_exists, index=False)\n",
    "        return f\"{state} data has been added to {filename}!\"\n",
    "    except Exception as e:\n",
    "        return f\"Unable to add {state} data to {filename}. Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a30d9e9c-cec4-44b0-a0d6-f16b07d8b38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama data has been added to us_cities_pop.csv!\n",
      "Alaska data has been added to us_cities_pop.csv!\n",
      "Arizona data has been added to us_cities_pop.csv!\n",
      "Arkansas data has been added to us_cities_pop.csv!\n",
      "California data has been added to us_cities_pop.csv!\n",
      "Colorado data has been added to us_cities_pop.csv!\n",
      "Connecticut data has been added to us_cities_pop.csv!\n",
      "District-of-Columbia data has been added to us_cities_pop.csv!\n",
      "Delaware data has been added to us_cities_pop.csv!\n",
      "Florida data has been added to us_cities_pop.csv!\n",
      "Georgia data has been added to us_cities_pop.csv!\n",
      "Hawaii data has been added to us_cities_pop.csv!\n",
      "Idaho data has been added to us_cities_pop.csv!\n",
      "Illinois data has been added to us_cities_pop.csv!\n",
      "Indiana data has been added to us_cities_pop.csv!\n",
      "Iowa data has been added to us_cities_pop.csv!\n",
      "Kansas data has been added to us_cities_pop.csv!\n",
      "Kentucky data has been added to us_cities_pop.csv!\n",
      "Louisiana data has been added to us_cities_pop.csv!\n",
      "Maine data has been added to us_cities_pop.csv!\n",
      "Maryland data has been added to us_cities_pop.csv!\n",
      "Massachusetts data has been added to us_cities_pop.csv!\n",
      "Michigan data has been added to us_cities_pop.csv!\n",
      "Minnesota data has been added to us_cities_pop.csv!\n",
      "Mississippi data has been added to us_cities_pop.csv!\n",
      "Missouri data has been added to us_cities_pop.csv!\n",
      "Montana data has been added to us_cities_pop.csv!\n",
      "Nebraska data has been added to us_cities_pop.csv!\n",
      "Nevada data has been added to us_cities_pop.csv!\n",
      "New-Hampshire data has been added to us_cities_pop.csv!\n",
      "New-Jersey data has been added to us_cities_pop.csv!\n",
      "New-Mexico data has been added to us_cities_pop.csv!\n",
      "New-York data has been added to us_cities_pop.csv!\n",
      "North-Carolina data has been added to us_cities_pop.csv!\n",
      "North-Dakota data has been added to us_cities_pop.csv!\n",
      "Ohio data has been added to us_cities_pop.csv!\n",
      "Oklahoma data has been added to us_cities_pop.csv!\n",
      "Oregon data has been added to us_cities_pop.csv!\n",
      "Pennsylvania data has been added to us_cities_pop.csv!\n",
      "Rhode-Island data has been added to us_cities_pop.csv!\n",
      "South-Carolina data has been added to us_cities_pop.csv!\n",
      "South-Dakota data has been added to us_cities_pop.csv!\n",
      "Tennessee data has been added to us_cities_pop.csv!\n",
      "Texas data has been added to us_cities_pop.csv!\n",
      "Utah data has been added to us_cities_pop.csv!\n",
      "Vermont data has been added to us_cities_pop.csv!\n",
      "Virginia data has been added to us_cities_pop.csv!\n",
      "Washington data has been added to us_cities_pop.csv!\n",
      "West-Virginia data has been added to us_cities_pop.csv!\n",
      "Wisconsin data has been added to us_cities_pop.csv!\n",
      "Wyoming data has been added to us_cities_pop.csv!\n"
     ]
    }
   ],
   "source": [
    "#US states based on url naming convention for this source\n",
    "us_states = [\n",
    "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \n",
    "    \"District-of-Columbia\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \n",
    "    \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \n",
    "    \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n",
    "    \"New-Hampshire\", \"New-Jersey\", \"New-Mexico\", \"New-York\", \"North-Carolina\", \n",
    "    \"North-Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode-Island\", \n",
    "    \"South-Carolina\", \"South-Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \n",
    "    \"Virginia\", \"Washington\", \"West-Virginia\", \"Wisconsin\", \"Wyoming\"\n",
    "]\n",
    "\n",
    "for state in us_states:\n",
    "    get_city_pop(state, CITY_POP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
