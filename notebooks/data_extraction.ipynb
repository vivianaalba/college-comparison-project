{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a67813-3fc6-457a-9313-4d47ba3144ba",
   "metadata": {},
   "source": [
    "<h1>Data Extraction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7c85838-772e-4a5b-a788-3831ae991ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import csv\n",
    "import openpyxl\n",
    "import os\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c4e8f-2086-4088-b4c5-ca86829637a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file names\n",
    "SCHOOL_FED = \"federal_college_data.csv\"\n",
    "SCHOOL_FORBES = \"forbes_rankings.csv\"\n",
    "SCHOOL_RATINGS = \"myplan_rankings.csv\"\n",
    "CITY_ZIPS = \"us_cities_zip_county.csv\"\n",
    "CITY_ZIPS_JSON = \"us_cities_zip_county.json\"\n",
    "CITY_CRIME = \"us_cities_crime.csv\"\n",
    "# Zippopotam API used in the data_integration code to match zips to cities in the CITY_CRIME file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edc5a17-b7d8-4bce-bf0d-c249bd90f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_ABBREVS = {\n",
    "    \"alabama\": \"AL\",\n",
    "    \"alaska\": \"AK\",\n",
    "    \"arizona\": \"AZ\",\n",
    "    \"arkansas\": \"AR\",\n",
    "    \"california\": \"CA\",\n",
    "    \"colorado\": \"CO\",\n",
    "    \"connecticut\": \"CT\",\n",
    "    \"delaware\": \"DE\",\n",
    "    \"florida\": \"FL\",\n",
    "    \"georgia\": \"GA\",\n",
    "    \"hawaii\": \"HI\",\n",
    "    \"idaho\": \"ID\",\n",
    "    \"illinois\": \"IL\",\n",
    "    \"indiana\": \"IN\",\n",
    "    \"iowa\": \"IA\",\n",
    "    \"kansas\": \"KS\",\n",
    "    \"kentucky\": \"KY\",\n",
    "    \"louisiana\": \"LA\",\n",
    "    \"maine\": \"ME\",\n",
    "    \"maryland\": \"MD\",\n",
    "    \"massachusetts\": \"MA\",\n",
    "    \"michigan\": \"MI\",\n",
    "    \"minnesota\": \"MN\",\n",
    "    \"mississippi\": \"MS\",\n",
    "    \"missouri\": \"MO\",\n",
    "    \"montana\": \"MT\",\n",
    "    \"nebraska\": \"NE\",\n",
    "    \"nevada\": \"NV\",\n",
    "    \"new hampshire\": \"NH\",\n",
    "    \"new jersey\": \"NJ\",\n",
    "    \"new mexico\": \"NM\",\n",
    "    \"new york\": \"NY\",\n",
    "    \"north carolina\": \"NC\",\n",
    "    \"north dakota\": \"ND\",\n",
    "    \"ohio\": \"OH\",\n",
    "    \"oklahoma\": \"OK\",\n",
    "    \"oregon\": \"OR\",\n",
    "    \"pennsylvania\": \"PA\",\n",
    "    \"rhode island\": \"RI\",\n",
    "    \"south carolina\": \"SC\",\n",
    "    \"south dakota\": \"SD\",\n",
    "    \"tennessee\": \"TN\",\n",
    "    \"texas\": \"TX\",\n",
    "    \"utah\": \"UT\",\n",
    "    \"vermont\": \"VT\",\n",
    "    \"virginia\": \"VA\",\n",
    "    \"washington\": \"WA\",\n",
    "    \"west virginia\": \"WV\",\n",
    "    \"wisconsin\": \"WI\",\n",
    "    \"wyoming\": \"WY\",\n",
    "    \"district of columbia\": \"DC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9c8b2-849e-4bc0-82cb-5682a7163e93",
   "metadata": {},
   "source": [
    "<h2>Validate CSV File Name</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e261f3d8-4a64-4ef4-85eb-48abe5829d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_filename(filename: str) -> bool:\n",
    "\n",
    "    # check for correct file extension\n",
    "    if not filename.lower().endswith(\".csv\"):\n",
    "        print(\"Error: Filename must end with .csv\")\n",
    "        return False\n",
    "\n",
    "    # check for invalid characters\n",
    "    invalid_characters = r'[<>:\"/\\\\|?*\\']'\n",
    "    if re.search(invalid_characters, filename):\n",
    "        print(\"Error: Filename contains invalid characters.\")\n",
    "        return False\n",
    "\n",
    "    # check for empty or whitespace-only name\n",
    "    if filename.strip() == \".csv\":\n",
    "        print(\"Error: Filename cannot be empty or just whitespace.\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73966f01-9d5a-4a89-8d7a-08ad50e19cae",
   "metadata": {},
   "source": [
    "<h2>Forbes Rankings Data</h2>\n",
    "<h3>Variables of Interest</h3>\n",
    "Institution Name, Rank, State, Average Grade, Median Base Salary, Student Population, Campus Setting, School Size, Description, Institution Type, Carnegie Classification, Student to Faculty Ratio, Total Grant Aid, Percent of Students Receive Financial Aid, Percent of Students Receive Grants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e060eb-5851-452c-bfa0-0c11d0e344cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forbes_data(filename: str):\n",
    "    # JSON API endpoint url from https://www.forbes.com/top-colleges/\n",
    "    FORBES_URL = \"https://www.forbes.com/forbesapi/org/top-colleges/2025/rank/true.json?fields=organizationName,academics,state,financialAid,rank,medianBaseSalary,campusSetting,studentPopulation,squareImage,uri,description,grade,schoolSize&limit=500&start=0\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    universities = []\n",
    "    \n",
    "    # validate filename to ensure that the data can be downloaded into a CSV file\n",
    "    if not validate_filename(filename):\n",
    "        return f\"Invalid filename: {filename}. Please check your filename and try again.\"\n",
    "    \n",
    "    # call page and show error if unable to make the request\n",
    "    try:\n",
    "        response = requests.get(FORBES_URL, headers=headers)\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e: # helps diagnose issues with HTTP request\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f\"JSON decoding error: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            schools = data['organizationList']['organizationsLists']\n",
    "        except KeyError as e: # helps diagnose API structure changes\n",
    "            print(f\"Key error: {e}\")\n",
    "            print(\"Available keys in response:\", data.keys())\n",
    "            return None\n",
    "\n",
    "    \n",
    "    # loop through schools and collect data\n",
    "    for school in schools:\n",
    "        university = {\n",
    "            'rank': school.get('rank'),\n",
    "            'name': school.get('organizationName'),\n",
    "            'state': school.get('state'),\n",
    "            'grade': school.get('grade'),\n",
    "            'medianBaseSalary': school.get('medianBaseSalary'),\n",
    "            'studentPopulation': school.get('studentPopulation'),\n",
    "            'campusSetting': school.get('campusSetting'),\n",
    "            'schoolSize': school.get('schoolSize'),\n",
    "            'description': school.get('description'),\n",
    "            'uri': school.get('uri')\n",
    "        }\n",
    "    \n",
    "        academics = school.get('academics', {})\n",
    "        university['institutionType'] = academics.get('type')\n",
    "        university['carnegieClassification'] = academics.get('carnegieClassification')\n",
    "        university['studentFacultyRatio'] = academics.get('studentFacultyRatio')\n",
    "    \n",
    "        financial_aid = school.get('financialAid', {})\n",
    "        university['totalGrantAid'] = financial_aid.get('totalGrantAid')\n",
    "        university['percentOfStudentsFinAid'] = financial_aid.get('percentOfStudentsFinAid')\n",
    "        university['percentOfStudentsGrant'] = financial_aid.get('percentOfStudentsGrant')\n",
    "        \n",
    "        # add all institution data to universities list, which can then be appended to a CSV file\n",
    "        universities.append(university)\n",
    "    \n",
    "    # create dataframe from university data and extract into a CSV file\n",
    "    df = pd.DataFrame(universities)\n",
    "    df.to_csv(f\"../data/{filename}\", index=False)\n",
    "\n",
    "    return f\"{len(universities)} universities added to {filename}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dd1d838-9ba7-4442-9297-a0ad03733ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'500 universities added to forbes_rankings.csv.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_forbes_data(SCHOOL_FORBES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6883a4d-ad07-47aa-b4b0-c825bee625df",
   "metadata": {},
   "source": [
    "<h2>myPlan Data</h2>\n",
    "<h3>Variables of Interest</h3>\n",
    "Institution Name, Prestige, Satisfaction, Resources & Facilities, Personal Safety, Teacher Support and Involvement, School Administration, Campus Setting, Aggregate score of all variables (Average Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "696bb889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "413ecd99-4daa-4166-bb71-8126f48318f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_myplan_data(var_name, url_num, entries: int):\n",
    "    # offset used to browse through page URLs, an offset of 400 lists schools ranked 401-500\n",
    "    url = f\"https://www.myplan.com/education/colleges/college_rankings_{url_num}.php?sort=1&offset=\"\n",
    "    university_rankings = []\n",
    "    \n",
    "    for i in range(0, entries + 100, 100):\n",
    "        page_url = url + str(i)\n",
    "\n",
    "        # access url\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Prevents blocking\n",
    "        html = requests.get(page_url, headers=headers, verify=False).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # loacte part of the HTML structure that contains university data\n",
    "        td_element = soup.find(\"td\", {\"background\": \"../../images/career_details_panel_bg_long.gif\"})\n",
    "\n",
    "        if td_element:\n",
    "            rows = td_element.find_all(\"tr\")\n",
    "            for row in rows:\n",
    "                cells = row.find_all(\"td\")\n",
    "                if len(cells) >= 3: \n",
    "                    name_tag = cells[1].find(\"a\", class_=\"ratings_list\")\n",
    "                    score_tag = cells[2].find(\"div\", align=\"right\")\n",
    "                    \n",
    "                    if name_tag and score_tag:\n",
    "                        name = name_tag.get_text(strip=True)\n",
    "                        score = score_tag.get_text(strip=True)\n",
    "                        try: #skip invalid scores\n",
    "                            float(score)\n",
    "                            university_rankings.append((name, score))\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "        else:\n",
    "            print(f\"No data found for offset {i}\")\n",
    "    \n",
    "    print(f\"{len(university_rankings)} total entries were found and documented for {var_name}.\")\n",
    "    return university_rankings\n",
    "\n",
    "\n",
    "# merge data from all school variables into a csv sheet\n",
    "# calculate avg score based on all variables\n",
    "def get_myplan_data(data_source: list, filename: str):\n",
    "\n",
    "    # scrape data\n",
    "    headers, variables_list = [], []\n",
    "    for var, url_num, entries in data_source:\n",
    "        ranking_list = scrape_myplan_data(var, url_num, entries)\n",
    "        headers.append(var)\n",
    "        variables_list.append(ranking_list)\n",
    "    \n",
    "\n",
    "    # validate CSV file name\n",
    "    if not validate_filename(filename):\n",
    "        return f\"Invalid filename: {filename}. Please check your filename and try again.\"\n",
    "\n",
    "    num_variables = len(headers)\n",
    "    merged = {}\n",
    "\n",
    "    # make a list of scores for each school\n",
    "    for var, ranking_list in enumerate(variables_list):\n",
    "        for school, score in ranking_list:\n",
    "            if school not in merged:\n",
    "                merged[school] = [None] * num_variables # initialize as None to track missing data\n",
    "            merged[school][var] = float(score)\n",
    "\n",
    "    # construct final merged list with averages\n",
    "    merged_list = []\n",
    "    for school, scores in merged.items():\n",
    "        non_none_scores = [s for s in scores if s is not None]\n",
    "        avg_score = round(sum(non_none_scores) / len(non_none_scores), 2) if non_none_scores else None\n",
    "        merged_list.append((school, *scores, avg_score))\n",
    "\n",
    "    # ensure data folder exists\n",
    "    os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "    with open(f\"../data/{filename}\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"school name\", *headers, \"average\"])\n",
    "        writer.writerows(merged_list)\n",
    "\n",
    "    print(f\"Saved to '../data/{filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56d63190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613 total entries were found and documented for overall satisfaction.\n",
      "613 total entries were found and documented for campus setting.\n",
      "610 total entries were found and documented for student housing.\n",
      "611 total entries were found and documented for resources.\n",
      "612 total entries were found and documented for safety.\n",
      "611 total entries were found and documented for teacher support.\n",
      "610 total entries were found and documented for school admin.\n",
      "611 total entries were found and documented for prestige.\n",
      "612 total entries were found and documented for student competitiveness.\n",
      "612 total entries were found and documented for student intelligence.\n",
      "506 total entries were found and documented for party scene.\n",
      "505 total entries were found and documented for greek life.\n",
      "506 total entries were found and documented for student attractiveness.\n",
      "Saved to '../data/myplan_rankings.csv'\n"
     ]
    }
   ],
   "source": [
    "# get myPlan ranking data from unique page per variable\n",
    "\n",
    "variable_sources = [\n",
    "    # category, url_num, #entries\n",
    "    (\"overall satisfaction\", 1, 613),\n",
    "    (\"campus setting\", 2, 613),\n",
    "    (\"student housing\", 3, 610),\n",
    "    (\"resources\", 4, 611),\n",
    "    (\"safety\", 5, 612),\n",
    "    (\"teacher support\", 6, 611),\n",
    "    (\"school admin\", 7, 610),\n",
    "    (\"prestige\", 8, 611),\n",
    "    (\"student competitiveness\", 9, 612),\n",
    "    (\"student intelligence\", 10, 612),\n",
    "    (\"party scene\", 11, 506),\n",
    "    (\"greek life\", 12, 505),\n",
    "    (\"student attractiveness\", 13, 506 )\n",
    "]\n",
    "\n",
    "variable_sources1 = [\n",
    "    # category, url_num, #entries\n",
    "    (\"overall satisfaction\", 1, 613),\n",
    "    (\"campus setting\", 2, 613)\n",
    "]\n",
    "\n",
    "get_myplan_data(variable_sources, SCHOOL_RATINGS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25714d0c-9d3b-4a9c-b20b-ee035df45d75",
   "metadata": {},
   "source": [
    "<h2>Department of Education API</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a4954b3-0b7a-4a9c-b713-f41fd96ff6c0",
   "metadata": {},
   "source": [
    "<h3>Variables Of Interest</h3>\n",
    "Name - name - INSTNM (TEXT) <br>\n",
    "State - school.state (TEXT) <br>\n",
    "Admission rate - admission_rate.overall - ADM_RATE (FLOAT) <br>  \n",
    "SAT Scores - admissions.sat_scores.average.overall <br>\n",
    "Enrollment of all undergraduate students - enrollment.all - UG (INT)  <br> \n",
    "Average net price for Title IV institutions (public institutions) - avg_net_price.public - NPT4_PUB (INT) <br>  \n",
    "Average net price for Title IV institutions (private for-profit and nonprofit institutions) - avg_net_price.private - NPT4_PRIV (INT)<br> \n",
    "Average cost of attendance (academic year institutions) - attendance.academic_year - COSTT4_A (INT)<br> \n",
    "The median debt for students who have completed - median_debt.completers.overall - GRAD_DEBT_MDN (FLOAT)<br> \n",
    "Median earnings of students working and not enrolled 8 years after entry - 8_yrs_after_entry.median_earnings - MD_EARN_WNE_P8 (FLOAT) <br>  \n",
    "Mean earnings of students working and not enrolled 8 years after entry - 8_yrs_after_entry.mean_earnings - MN_EARN_WNE_P8 (FLOAT)  <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b0bef-3a57-4106-9a9e-deb568083371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request\n",
    "# https://api.data.gov/ed/collegescorecard/v1/schools?api_key=YOUR_API_KEY&fields=id,school.name,latest.cost.tuition.in_state,latest.completion.rate\n",
    "\n",
    "# to request API key, visit: https://collegescorecard.ed.gov/data/api-documentation\n",
    "# API default rate limit - 1,000 requests per IP address per hour\n",
    "load_dotenv()  # Load from .env\n",
    "secret_key = os.getenv(\"API_KEY\")\n",
    "API_KEY = secret_key\n",
    "BASE_URL = \"https://api.data.gov/ed/collegescorecard/v1/schools\"\n",
    "\n",
    "# variables of interest [add here to include additional variables in research]\n",
    "# all variables can be found in ../data/CollegeScorecardDataDictionary.xlsx\n",
    "FIELDS = [\n",
    "    \"school.zip\",\n",
    "    \"school.name\",\n",
    "    \"school.sector.scorecard\",\n",
    "    \"school.city\",\n",
    "    \"school.state\",\n",
    "    \"latest.school.state_fips\",\n",
    "    \"latest.school.locale\",\n",
    "    \"latest.school.faculty_salary\",\n",
    "    \"latest.admissions.sat_scores.average.overall\",\n",
    "    \"latest.admissions.admission_rate.overall\",\n",
    "    \"latest.completion.title_iv.completed_by.4yrs\",\n",
    "    \"latest.cost.avg_net_price.public\",\n",
    "    \"latest.cost.avg_net_price.private\",\n",
    "    \"latest.cost.attendance.academic_year\",\n",
    "    \"latest.aid.median_debt.completers.overall\",\n",
    "    \"latest.earnings.8_yrs_after_entry.median_earnings\",\n",
    "    \"latest.earnings.8_yrs_after_entry.mean_earnings\",\n",
    "    \"latest.student.retention_rate.four_year.full_time_pooled\",\n",
    "    \"latest.student.demographics.student_faculty_ratio\"\n",
    "]\n",
    "\n",
    "HEADERS = [\n",
    "    \"Zip Code\",\n",
    "    \"Name\",\n",
    "    \"Type\", # labeled in institution_types\n",
    "    \"City\",\n",
    "    \"State\",\n",
    "    \"State FIPS\",\n",
    "    \"Locale\",\n",
    "    \"Average Faculty Salary\",\n",
    "    \"Average SAT Score\",\n",
    "    \"Admission Rate\",\n",
    "    \"4-Year Completion Rate\",\n",
    "    \"Average Net Price (Public)\",\n",
    "    \"Average Net Price (Private)\",\n",
    "    \"Cost of Attendance (Academic Year)\",\n",
    "    \"Median Debt of Completers\",\n",
    "    \"Median Earnings (8 Years After Entry)\",\n",
    "    \"Mean Earnings (8 Years After Entry)\",\n",
    "    \"Retention Rate\",\n",
    "    \"Student Faculty Ratio\"\n",
    "]\n",
    "\n",
    "institution_types = {\n",
    "    1: \"Graduate-only Public institution\",\n",
    "    2: \"Graduate-only Nonprofit institution\",\n",
    "    3: \"Graduate-only For-profit institution\",\n",
    "    4: \"4-year Public institution\",\n",
    "    5: \"4-year Nonprofit institution\",\n",
    "    6: \"4-year For-profit institution\",\n",
    "    7: \"2-year Public institution\",\n",
    "    8: \"2-year Nonprofit institution\",\n",
    "    9: \"2-year For-profit institution\",\n",
    "    10: \"Less-than-2-year Public institution\",\n",
    "    11: \"Less-than-2-year Nonprofit institution\",\n",
    "    12: \"Less-than-2-year For-profit institution\",\n",
    "    13: \"Unknown Public institution\",\n",
    "    14: \"Unknown Nonprofit institution\",\n",
    "    15: \"Unknown For-profit institution\"\n",
    "}\n",
    "\n",
    "def get_data_to_csv(start_page, end_page, filename):\n",
    "    # validate CSV file name\n",
    "    if not validate_filename(filename):\n",
    "        return f\"Invalid filename: {filename}. Please check your filename and try again.\"\n",
    "    \n",
    "    all_data = []\n",
    "    page = start_page\n",
    "    \n",
    "    while page < end_page:\n",
    "        params = {\n",
    "            \"api_key\": API_KEY,\n",
    "            \"fields\": \",\".join(FIELDS),\n",
    "            \"per_page\": 100,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        results = data.get(\"results\", [])\n",
    "\n",
    "        def safe_get(school, field):\n",
    "            value = school.get(field)\n",
    "            return value if value is not None else \"N/A\"\n",
    "        \n",
    "        for school in results:\n",
    "            row = [safe_get(school, field) for field in FIELDS]\n",
    "            all_data.append(row)\n",
    "\n",
    "        page += 1\n",
    "    \n",
    "    # after fetching all data, save to CSV with human readable headers\n",
    "    # there are thousands of institutions in this API, so we will extract data in portions to check for error\n",
    "    # use append mode to add data at the end of the CSV file to prevent overwriting previous entries\n",
    "    file_exists = os.path.isfile(f\"../data/{filename}\") # use os to prevent rewriting headers\n",
    "    with open(f\"../data/{filename}\", \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if not file_exists:\n",
    "            writer.writerow(HEADERS)\n",
    "        writer.writerows(all_data)\n",
    " \n",
    "    print(f\"Data from pages {start_page} to {end_page} saved to '../data/{filename}' - {len(all_data)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a45846-4d3a-415b-afab-9a5261a73e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from pages 0 to 66 saved to '../data/federal_college_data.csv' - 6429 records\n"
     ]
    }
   ],
   "source": [
    "# 6429 school entries total as of 7/31/2025\n",
    "get_data_to_csv(0, 66, SCHOOL_FED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82d80f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns missing in every row:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# check that all columns have at least some data\n",
    "\n",
    "fed_df = pd.read_csv(\"../data/federal_college_data.csv\", na_values=[\"N/A\"])\n",
    "\n",
    "print(\"Columns missing in every row:\")\n",
    "print(fed_df.columns[fed_df.isna().all()].tolist()) # none, meaning that all variables were extracted properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09502e6d-0761-45a1-b0f6-dc6951e84643",
   "metadata": {},
   "source": [
    "<h2>US City Population Data</h2>\n",
    "US Cities by State and their population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3280fec5-b3b4-4ccd-94af-40d71efca296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_info(filename):\n",
    "    # validate CSV file name\n",
    "    # validate JSON filename\n",
    "    if not filename.endswith(\".json\"):\n",
    "        return f\"Invalid filename: {filename}. Must end in '.json'.\"\n",
    "    \n",
    "    all_data = []\n",
    "\n",
    "    for state, abbrev in STATE_ABBREVS.items():\n",
    "        url = f\"https://www.geonames.org/postal-codes/US/{abbrev}/{state}.html\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch data for {state}: {e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        table = soup.find(\"table\", {\"class\": \"restable\"})\n",
    "        if not table:\n",
    "            print(f\"No data table found for {state}\")\n",
    "            continue\n",
    "\n",
    "        rows = table.find_all(\"tr\")[1:]  # Skip header row\n",
    "\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if len(cells) >= 6:\n",
    "                city = cells[1].get_text(strip=True)\n",
    "                postal_code = cells[2].get_text(strip=True)\n",
    "                state_full = cells[4].get_text(strip=True)\n",
    "                county = cells[5].get_text(strip=True)\n",
    "\n",
    "                all_data.append({\n",
    "                    \"City\": city,\n",
    "                    \"Postal Code\": postal_code,\n",
    "                    \"State\": state_full,\n",
    "                    \"County\": county,\n",
    "                })\n",
    "\n",
    "        output_path = f\"../data/{filename}\"\n",
    "        try:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "            return f\"Data successfully written to '{output_path}'.\"\n",
    "        except Exception as e:\n",
    "            return f\"Failed to write data to JSON: {e}\"\n",
    "    \n",
    "    return f\"Available data was added to '../data/{filename}'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810c078-2128-4975-91e9-333ec2b74522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Data successfully written to '../data/us_cities_zip_county.json'.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data as json\n",
    "get_city_info(CITY_ZIPS_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a4b284",
   "metadata": {},
   "source": [
    "<h2>US Crime Data</h2>\n",
    "Excel download available from the \"Offenses Known To Law Enforcement” dataset by the Federal Bureau of Investigation (FBI) Crime Data ExplorerData - https://cde.ucr.cjis.gov/LATEST/webapp/#/pages/home (By State and University)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_merged_cells(excel_file, sheet_name=0):\n",
    "    wb = openpyxl.load_workbook(excel_file)\n",
    "    sheet = wb[sheet_name] if isinstance(sheet_name, str) else wb.worksheets[sheet_name]\n",
    "\n",
    "    # Expand merged cells (copy the value to all cells in the merged range)\n",
    "    for merged_range in sheet.merged_cells.ranges:\n",
    "        min_row, min_col, max_row, max_col = merged_range.bounds\n",
    "        value = sheet.cell(row=min_row, column=min_col).value\n",
    "        for row in range(min_row, max_row + 1):\n",
    "            for col in range(min_col, max_col + 1):\n",
    "                sheet.cell(row=row, column=col).value = value\n",
    "\n",
    "    # Extract data into a list of lists\n",
    "    data = []\n",
    "    for row in sheet.iter_rows(values_only=True):\n",
    "        data.append(list(row))\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df = expand_merged_cells(\"your_file.xlsx\", sheet_name=0)\n",
    "df.to_csv(\"flattened_output.csv\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
